ðŸš€ Ultimate Prompt Template for Decomposing Complex Coding Tasks
This template is designed to guide Large Language Models (LLMs) in breaking down complex coding tasks into manageable, modular components while ensuring clarity, maintainability, and scalability. It incorporates structured decomposition, advanced prompting techniques, and best practices tailored for software engineering, data engineering, and data science projects.

Phase 1: Define the Task and Context

Instruction: Clearly state the specific coding objective in precise terms.
Example: "Develop a Python function to process log files and calculate user activity metrics."


Context: Provide essential technical details to frame the task.
Programming language and version: e.g., Python 3.11
Frameworks or libraries: e.g., Use Pandas and SQLAlchemy; avoid NumPy
Execution environment: e.g., Local machine with 16GB RAM
Data characteristics: e.g., Up to 1 million rows of CSV data
Example: "Use Python 3.11 with Pandas in a local environment. Process CSV log files containing up to 1 million rows."




Phase 2: Decompose the Task into Modular Components

Strategy: Break the problem into smaller, independent units with single responsibilities (following the Single Responsibility Principle).
Technique: Apply functional decomposition or SOLID principles to identify components.
Example Components:
load_logs(directory: str) -> pd.DataFrame: Reads log files.
normalize_data(df: pd.DataFrame) -> pd.DataFrame: Cleans and standardizes data.
aggregate_metrics(df: pd.DataFrame) -> pd.DataFrame: Calculates user metrics.




Interfaces: Define clear inputs and outputs for each component.
Example: normalize_data(df: pd.DataFrame) -> pd.DataFrame expects a DataFrame with raw logs and returns a cleaned DataFrame.




Phase 3: Plan the Workflow and Dependencies

Order of Operations: Specify the sequence or parallel execution paths.
Example: Load â†’ Normalize â†’ Aggregate


Dependencies: Identify inter-component relationships.
Example: aggregate_metrics() depends on normalize_data() output.


Configuration: Externalize settings (e.g., file paths, credentials) via a config file or environment variables.
Example: Use config.yaml for file paths and database URLs.




Phase 4: Anticipate Challenges and Constraints

Potential Roadblocks: List foreseeable issues per component.
Example: 
Load: Missing files or malformed CSV.
Normalize: Inconsistent data formats.




Constraints: Define performance, resource, or scalability limits.
Example: "Process 1 million rows in under 10 minutes with 16GB RAM."




Phase 5: Design Testing and Validation

Unit Testing: Plan tests for each component.
Example: Test normalize_data() with a DataFrame containing null values.


Integration Testing: Verify the full workflow.
Example: Test the pipeline end-to-end with a sample log file.


Validation: Ensure data integrity and quality.
Example: Check for no duplicate records in the output.




Phase 6: Ensure Documentation and Code Quality

Inline Comments: Explain complex logic within the code.
Docstrings: Document each function or class.
Example:def load_logs(directory: str) -> pd.DataFrame:
    """Loads CSV log files from a directory into a DataFrame.
    
    Args:
        directory (str): Path to log files.
    
    Returns:
        pd.DataFrame: Combined log data.
    """




External Docs: Plan a README or architecture diagram if needed.


Phase 7: Version Control and Resources

Version Control: Use Git for tracking changes.
Example: Create branches like feature/load-logs.


Resources: List helpful references.
Example: Pandas CSV parsing docs, Stack Overflow threads on large file handling.




Phase 8: Set a Timeline

Milestones: Define deadlines for each component.
Example: 
Load: 2 days
Normalize: 3 days
Aggregate: 3 days






Phase 9: Iterative Refinement

Review: After implementation, critique the solution.
Technique: Use Recursive Criticism and Improvement (RCI).
Example: "Review the code for performance bottlenecks and suggest optimizations."




ðŸ§  Advanced Prompting Techniques

Chain-of-Thought (CoT): "Break the task into steps and explain each before coding."
Role Prompting: "Act as a senior software engineer designing a modular pipeline."
Few-shot Examples: 
Example: "Input: ['2023-01-01,user1,click,5'] â†’ Output: Cleaned DataFrame row."


Self-Consistency: "Generate two solutions and compare their efficiency."


ðŸ“š Complete Prompt Example

Task: "Create a Python script to process CSV log files, normalize the data, and compute user activity metrics."
Context: "Use Python 3.11 with Pandas. Process logs with up to 1 million rows on a local machine."
Decomposition:

load_logs(directory: str) -> pd.DataFrame: Load CSV files.
normalize_data(df: pd.DataFrame) -> pd.DataFrame: Clean data (timestamps, IDs).
aggregate_metrics(df: pd.DataFrame) -> pd.DataFrame: Compute metrics (e.g., click counts).

Workflow: Load â†’ Normalize â†’ Aggregate
Challenges: Handle missing files, inconsistent formats.
Testing: Unit tests for each function; integration test with sample data.
Documentation: Include docstrings and inline comments.
Version Control: Use Git with feature branches.
Resources: Pandas docs, Git tutorials.
Timeline: Complete in 8 days (Load: 2, Normalize: 3, Aggregate: 3).
Refinement: "After coding, review for efficiency and optimize."
Prompting: "Act as a senior Python engineer. Break this into steps, write the code, and test it."


This template provides a systematic, reusable approach to decomposing complex coding tasks, ensuring modular design and robust solutions.
